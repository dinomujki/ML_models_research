{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mujki/anaconda3/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Goal\\u200b : delete all manual setup to build a goal-oriented bot (document creation, price \\nestimation, etc…) \\n \\nProblem\\u200b : the manual setup of those kind of chatbots is time-consuming and error-prone.  \\nNo scripted solutions seem to be efficient enough to reduce setup duration and errors.  \\nWe always turn around a useless question : is it better to use filters or gotoressources / bots.  \\nCan be explained deeper if needed.  \\n \\nHypothesis\\u200b : switching the way we see the problem could offer a solution.  \\nThe final goal of these chatbots is to perform an action.  \\nLet’s say the action is creation a contract for a bank’s corporate customers (guarantee \\ncontracts). \\n \\nLet’s also say we have access to 10k+ of those contracts. \\n \\nHere’s our hypothesis (so the question for Dino) :  \\n \\n\\n1) Know the list of attributes needed \\n2) Turn those 10k+ pdf files into text files (OCR, some open source stuff should exist) \\n3) Manually train the NLU to extract entities (needed attributes) from a sample of this \\n\\nsource of contract \\n\\n4) Apply the created algorithm to the 10k examples \\n5) Develop a Neural Network able to score the probability of needing an attribute \\n\\ncompared to one another \\n\\n6) Setup the bot and run the logic from the created NN \\n\\n \\n \\nWe can provide: \\n\\n-\\n-\\n\\nthe list of attribute needed \\na trained NLU to extract the entities from the PDF \\n\\n \\nNeed to be done: \\n\\n- OCR: PDF to Text \\n- Entities extractor that can run on 10k text files \\n- Create the NN for the logic of the bot \\n\\n\\x0c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = './data/test.pdf'\n",
    "text = convert_pdf_to_txt(test_file)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goal : delete all manual setup to build a goal-oriented bot (document creation, price',\n",
       " 'estimation, etc)',\n",
       " 'Problem : the manual setup of those kind of chatbots is time-consuming and error-prone.',\n",
       " 'No scripted solutions seem to be efficient enough to reduce setup duration and errors.',\n",
       " 'We always turn around a useless question : is it better to use filters or gotoressources / bots.',\n",
       " 'Can be explained deeper if needed.',\n",
       " 'Hypothesis : switching the way we see the problem could offer a solution.',\n",
       " 'The final goal of these chatbots is to perform an action.',\n",
       " 'Lets say the action is creation a contract for a banks corporate customers (guarantee',\n",
       " 'contracts).',\n",
       " 'Lets also say we have access to 10k+ of those contracts.',\n",
       " 'Heres our hypothesis (so the question for Dino) :',\n",
       " '1) Know the list of attributes needed',\n",
       " '2) Turn those 10k+ pdf files into text files (OCR, some open source stuff should exist)',\n",
       " '3) Manually train the NLU to extract entities (needed attributes) from a sample of this',\n",
       " 'source of contract',\n",
       " '4) Apply the created algorithm to the 10k examples',\n",
       " '5) Develop a Neural Network able to score the probability of needing an attribute',\n",
       " 'compared to one another',\n",
       " '6) Setup the bot and run the logic from the created NN',\n",
       " 'We can provide:',\n",
       " '-',\n",
       " '-',\n",
       " 'the list of attribute needed',\n",
       " 'a trained NLU to extract the entities from the PDF',\n",
       " 'Need to be done:',\n",
       " '- OCR: PDF to Text',\n",
       " '- Entities extractor that can run on 10k text files',\n",
       " '- Create the NN for the logic of the bot']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''.join(x for x in text if x in string.printable)\n",
    "lines = text.split('\\n')\n",
    "lines = list(map(lambda x: x.strip(), lines))\n",
    "lines = list(filter(None, lines))\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Goal : delete all manual setup to build a goal-oriented bot (document creation, priceestimation, etc)',\n",
       " 'Problem : the manual setup of those kind of chatbots is time-consuming and error-prone.',\n",
       " 'No scripted solutions seem to be efficient enough to reduce setup duration and errors.',\n",
       " 'We always turn around a useless question : is it better to use filters or gotoressources / bots.',\n",
       " 'Can be explained deeper if needed.',\n",
       " 'Hypothesis : switching the way we see the problem could offer a solution.',\n",
       " 'The final goal of these chatbots is to perform an action.',\n",
       " 'Lets say the action is creation a contract for a banks corporate customers (guaranteecontracts).',\n",
       " 'Lets also say we have access to 10k+ of those contracts.',\n",
       " 'Heres our hypothesis (so the question for Dino) :',\n",
       " '1) Know the list of attributes needed',\n",
       " '2) Turn those 10k+ pdf files into text files (OCR, some open source stuff should exist)',\n",
       " '3) Manually train the NLU to extract entities (needed attributes) from a sample of thissource of contract',\n",
       " '4) Apply the created algorithm to the 10k examples',\n",
       " '5) Develop a Neural Network able to score the probability of needing an attributecompared to one another',\n",
       " '6) Setup the bot and run the logic from the created NN',\n",
       " 'We can provide:',\n",
       " '-',\n",
       " '-the list of attribute neededa trained NLU to extract the entities from the PDF',\n",
       " 'Need to be done:',\n",
       " '- OCR: PDF to Text',\n",
       " '- Entities extractor that can run on 10k text files']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved = None\n",
    "data = []\n",
    "for line in lines:\n",
    "    if saved == None:\n",
    "        saved = line\n",
    "        continue\n",
    "    if line[0].islower():\n",
    "        saved += line\n",
    "    else:\n",
    "        data.append(saved)\n",
    "        saved = line\n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': None,\n",
       " 'entities': [{'start': 0,\n",
       "   'end': 4,\n",
       "   'value': 'test',\n",
       "   'entity': 'name',\n",
       "   'confidence': 0.8660116913795302,\n",
       "   'extractor': 'ner_crf'}],\n",
       " 'intent_ranking': [],\n",
       " 'text': 'test',\n",
       " 'project': 'current',\n",
       " 'model': 'model_20190131-142422'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'query': 'test', 'project': 'current'}\n",
    "r = requests.post(\"http://localhost:5000/parse\", json=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_entities = {}\n",
    "for line in lines:\n",
    "    payload = {'query': line, 'project': 'current'}\n",
    "    r = requests.post(\"http://localhost:5000/parse\", json=payload)\n",
    "    if r.status_code != 200:\n",
    "        continue\n",
    "        \n",
    "    response = r.json()\n",
    "    for entityObj in response['entities']:\n",
    "        entity = entityObj['entity']\n",
    "        value = entityObj['value']\n",
    "        doc_entities[entity] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'bot'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_data = [doc_entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bot', None, None]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entities = ['name', 'value', 'currency']\n",
    "docs = []\n",
    "for doc_entities in docs_data:\n",
    "    temp = []\n",
    "    for entity in all_entities:\n",
    "        if entity not in doc_entities:\n",
    "            temp.append(None)\n",
    "        else:\n",
    "            temp.append(doc_entities[entity])\n",
    "    docs.append(temp)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = list(map(lambda x: list(map(lambda y: 1 if y!=None else 0, x)), docs))\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  value  currency\n",
       "0     1      0         0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(matrix, columns=['name', 'value', 'currency'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
